
---

# 배치 처리 워크플로우 구축 (Airflow + Spark)

이 문서는 Airflow와 Spark를 활용하여 대용량 데이터를 처리하는 배치 워크플로우를 구축하는 방법에 대해 다룹니다.

## 1. 배치 처리와 실시간 처리의 이해

### 배치 처리 (Batch Processing)란?

* **정의:** 일정량의 데이터를 모아서 한꺼번에 처리하는 방식입니다.
* **특징:**
* 정해진 시간(예: 매일 밤 12시) 또는 특정 이벤트(예: 파일 업로드) 발생 시 실행됩니다.
* 대량 데이터를 처리하는 데 적합하며, 주로 ETL 파이프라인, 데이터 웨어하우스 적재 등에 활용됩니다.


* **활용 사례:**
* 데이터 웨어하우스 적재 (예: Redshift, Snowflake)
* 사용자 로그 분석 (예: 하루 단위 사용자 접속 로그 집계)
* 기계 학습 모델 학습을 위한 데이터 준비



### 실시간 처리 (Real-time Processing)란?

* **정의:** 데이터가 생성되는 즉시 실시간으로 처리하는 방식입니다.
* **특징:**
* 지연 시간이 짧고, 스트리밍 데이터(예: 실시간 로그, IoT 센서 데이터) 처리에 최적화되어 있습니다.
* 주로 실시간 모니터링, 이상 탐지, 실시간 추천 시스템 등에 활용됩니다.



### 배치 처리 vs 실시간 처리 비교

| 항목 | 배치 처리 | 실시간 처리 |
| --- | --- | --- |
| **데이터 처리 방식** | 일정량의 데이터를 모아서 한 번에 처리 | 데이터가 들어오는 즉시 처리 |
| **처리 속도** | 느림 (분~시간 단위) | 빠름 (밀리초~초 단위) |
| **적용 사례** | 데이터 웨어하우스, 머신러닝 학습 데이터 준비 | 실시간 이상치 탐지, 실시간 추천 시스템, IoT 데이터 분석 |
| **리소스 사용** | 주어진 시간에만 리소스 사용 | 지속적인 리소스 사용 |
| **비용** | 상대적으로 저렴 (일정한 리소스 사용) | 고비용 |
| **사례** | 매일 오후 20시 로그 데이터 집계 | 실시간 주식 거래 분석 |

---

## 2. Airflow와 Spark를 활용한 데이터 처리

### Spark Data Structure

1. **RDD (Resilient Distributed Dataset)**
* 분산된 데이터를 저장하고 처리하는 기본 단위 (Hadoop의 HDFS와 유사)
* 변경 불가능 (Immutable) -> 안정적인 분산 처리를 지원
* 여러 노드에서 병렬로 처리 가능


2. **DataFrame (Pandas와 유사)**
* 구조화된 데이터(테이블 형태)를 처리하는 최적화된 데이터 구조
* Spark SQL과 연동 가능 -> SQL 쿼리를 사용하여 데이터 변환


3. **Spark SQL**
* SQL을 활용해 데이터를 쉽게 조회하고 변환 가능
* 다양한 데이터 소스(HDFS, S3, JDBC, Hive, Cassandra 등)에서 데이터를 가져올 수 있음



### Apache Spark의 처리 과정

* **RDD 배치 처리 과정:**
* `Data Source` -> `parallelize` -> `RDD` -> `Transform` -> `RDD` -> `Action`
* 데이터를 생성하고 Transformation 연산(map, filter 등)을 수행하여 새로운 RDD를 생성하지만 즉시 실행되지 않음(Lazy Execution). 마지막 Action 연산 시 DAG가 실행되어 실제 데이터 처리가 수행됨.


* **DataFrame 배치 처리 과정:**
* `Data Source` -> `read / load` -> `DataFrame` -> `Transform` -> `DataFrame` -> `Action`
* `select()`, `filter()`, `groupby()` 등의 Transformation이 체이닝 되며, `show()`, `count()`, `write()`와 같은 Action이 호출되어야 실제 실행이 이루어짐.



### Airflow에서 Spark를 활용하는 이유

* 단순 Python 코드로 처리하기 어려운 대규모 데이터 처리에 적합합니다.
* Pandas는 메모리에 로드할 수 있는 데이터 크기에 한계가 있지만, Spark는 병렬 처리가 가능하여 속도가 느려지지 않고 여러 노드에서 나눠서 처리하는 기능이 있습니다.
* **역할 분담:**
* **Airflow:** 복잡한 ETL 작업을 스케줄링하고 워크플로우를 관리(Orchestration).
* **Spark:** 대규모 데이터 변환 및 연산 수행(Compute).


* Spark 작업을 모니터링하고 실패 시 자동 재시도할 수 있습니다.

---

## 3. 배치 워크플로우에서 DAG의 역할

### DAG의 흐름 구성

* 병렬 태스크와 순차 태스크가 함께 구성된 DAG로, 데이터 로드부터 후속 처리까지 단계별 실행 흐름을 정의할 수 있습니다.
* 작업 간 의존성을 명확히 설정하고, 병렬 처리를 통해 전체 실행 시간을 단축할 수 있습니다.
* **구조 예시:** Start -> (Task_A1, Task_B1, Task_C1 병렬 실행) -> ... -> End

### DAG에서 BashOperator를 활용한 데이터 전처리 스크립트 실행

Shell로 작성된 데이터 전처리 스크립트를 `BashOperator`로 실행하는 예제 코드입니다.

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

# 기본 설정 정의
default_args = {
    'owner': 'admin',
    'start_date': datetime(2025, 3, 10),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# DAG 정의
with DAG(
    dag_id='bash_processing_dag',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
    tags=['bash', 'preprocessing']
) as dag:

    # BashOperator 정의
    process_data_task = BashOperator(
        task_id='run_bash_script',
        bash_command='/opt/airflow/plugins/shell/process_data.sh ',
        # process_data.sh 내부에 데이터 전처리 로직 포함
    )

    # 태스크 실행 순서 정의
    process_data_task

```

---

## 4. Airflow Connections 및 Hooks 소개

### Connection & Hook

* **Connection:** Airflow가 외부 시스템(DB, API, Cloud 등)에 접속하기 위한 정보를 저장하는 객체입니다. (Web UI 또는 환경 변수를 통해 관리)
* **Hook:** Connection을 사용하여 실제로 외부 시스템과 통신하는 인터페이스입니다. Operator에서 Hook을 활용하여 작업을 수행합니다.

### PostgreSQL Web UI Connection 설정 및 테스트 예시

`PostgresHook`을 사용하여 DB에 연결하고 데이터를 조회하는 코드입니다.

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
import pendulum

def fetch_postgres_data():
    # PostgresHook을 사용하여 연결 객체 생성
    hook = PostgresHook(postgres_conn_id='my_postgres_conn')
    conn = hook.get_conn()
    print(f"Connecting to host: {conn.host}, port: {conn.port}, db: {conn.schema}, user: {conn.login}")

    # with hook.get_conn() as conn: (위와 동일하게 사용 가능)
    with conn.cursor() as cur:
        cur.execute("SELECT 1;")
        res = cur.fetchone()[0]
        cur.execute("SELECT version();")
        version = cur.fetchone()[0]

    if res == 1:
        print(f"Postgres Connected! version: {version}")
    else:
        raise Exception("Postgres Connection Failed")

with DAG(
    dag_id="postgres_hook_python_operator",
    start_date=pendulum.datetime(2025, 3, 10, tz="Asia/Seoul"),
    schedule_interval="@daily",
    catchup=False,
    tags=["postgres", "hook"]
) as dag:
    
    fetch_data_task = PythonOperator(
        task_id="fetch_postgres_data",
        python_callable=fetch_postgres_data
    )

    fetch_data_task

```

---

## 5. DAG에서 SparkSubmitOperator 활용

### SparkSubmitOperator 개요

* Apache Airflow에서 Spark 애플리케이션을 실행하기 위한 전용 연산자입니다.
* Spark 클러스터(YARN, Kubernetes 등)에 `.py`, `.jar`, `.scala` 파일 등을 제출(submit)합니다.
* 복잡한 Spark 작업을 Airflow DAG 내에 통합하여 자동화된 데이터 파이프라인 구성이 가능합니다.

### docker-compose.yaml에 Spark 서비스 추가하기

Airflow와 연동하기 위해 Spark 마스터와 워커 컨테이너를 정의하는 설정입니다.

```yaml
# spark-master 설정
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8081:8080"  # Spark Master Web UI
      - "7077:7077"  # Spark Master Port
    networks:
      - airflow

# spark-worker 설정
  spark-worker:
    image: bitnami/spark:latest
    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    depends_on:
      - spark-master
    networks:
      - airflow

```

* **네트워크 설정:** `airflow` 네트워크를 공유하여 Airflow 컨테이너에서 Spark 마스터(`spark-master`)에 접근할 수 있도록 설정해야 합니다.

### Airflow UI에서 Spark Connection 추가

1. Airflow Web UI -> Admin -> Connections 메뉴 이동
2. `+` 버튼 클릭하여 새 Connection 생성
3. **Connection Id:** `spark_default`
4. **Connection Type:** `Spark`
5. **Host:** `spark://spark-master`
6. **Port:** `7077`
7. **Deploy Mode:** `client` (또는 cluster)
8. **Spark Binary:** `spark-submit`

### SparkSubmitOperator를 통한 Spark Job 실행 및 코드

실제 수행되는 Spark 애플리케이션 Python 코드(`spark_app.py`로 추정)입니다.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg

# Spark 세션 생성
spark = SparkSession.builder \
    .appName("DataFrameTest") \
    .getOrCreate()

# 샘플 데이터 생성
data = [
    ("James", "Sales", 3000),
    ("Michael", "Sales", 4600),
    ("Robert", "Sales", 4100),
    ("Maria", "Finance", 3000),
    ("James", "Sales", 3000),
    ("Scott", "Finance", 3300),
    ("Jen", "Finance", 3900),
    ("Jeff", "Marketing", 3000),
    ("Kumar", "Marketing", 2000),
    ("Saif", "Sales", 4100)
]

# DataFrame 생성
columns = ["Employee_Name", "Department", "Salary"]
df = spark.createDataFrame(data, columns)

# 데이터 처리: 부서별 평균 급여 계산
avg_salary_df = df.groupBy("Department").agg(avg("Salary").alias("avg_salary"))

# 결과 출력
avg_salary_df.show()

# Spark 세션 종료
spark.stop()

```

이 코드는 Airflow의 `SparkSubmitOperator`를 통해 실행되며, 실행 결과는 Airflow 로그에서 `Task Name`, `Finished` 상태 등으로 확인할 수 있습니다.

---

## 6. 오류 확인 및 디버깅

### DAG 실행 오류 확인 (Airflow Web UI 로그)

* DAG 실행 중 에러가 발생할 경우, 각 Task에 대한 상세 로그를 확인할 수 있습니다.
* **확인 방법:**
1. Airflow Web UI에서 실패한(빨간색) Task 클릭
2. 상단 메뉴의 `Log` 버튼 클릭


* 로그 내용 중 `Error` 또는 `Exception` 키워드를 찾아 원인을 분석합니다.
* **예시:** Python 코드 내에서 0으로 나누는 연산이 있을 경우 `ZeroDivisionError: division by zero`와 같은 Traceback이 로그에 남습니다. 이를 통해 코드의 어느 부분(Line number)에서 문제가 발생했는지 파악할 수 있습니다.

---
