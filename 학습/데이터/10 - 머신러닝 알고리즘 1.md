
---

# 데이터 분석을 위한 머신러닝 알고리즘 1

## 1. 챕터의 포인트

이 강의에서는 데이터 분석과 머신러닝의 기초가 되는 핵심 수학적 개념과 알고리즘을 다룹니다.

* **선형변환 및 행렬 연산**
* **Regression (회귀)**
* **베이즈 정리, 나이브 베이즈 (분류)**
* **Classification (분류)**
* **KNN (분류)**

---

## 2. 벡터와 행렬의 기초

### 머신러닝에서 행렬 연산이 필요한 이유

* **데이터를 효율적으로 표현:** 모델을 학습시키기 위해 행렬을 사용.
* **데이터 표현:** 대부분의 데이터(이미지, 텍스트, 수치 데이터)는 행렬 형태로 저장됨.
* **머신러닝 알고리즘:** 선형 회귀, 로지스틱 회귀, 신경망 등에서 가중치와 데이터 처리 연산 및 데이터 변형, 차원 축소를 위해 행렬 사용.
* **최적화 및 학습 과정:** 경사 하강법 등 최적화 알고리즘 및 딥러닝에서 미분과 병렬 연산 사용.

### 행렬 연산 기초 (덧셈, 스칼라 곱, 행렬 곱)

* **행렬 간 덧셈:** 같은 크기의 행렬끼리 요소별로 더함.
* **스칼라 곱:** 행렬의 각 요소에 숫자를 곱함.
* **행렬 곱 (Matrix Multiplication):** 두 개의 행렬을 곱하는 연산.
*  행렬과  행렬의 곱으로 표현하는 형태를 벡터의 내적이라 볼 수 있음.
* 머신러닝(선형 회귀, 신경망)에서 필수적으로 활용됨.



#### [Code] 행렬 덧셈과 스칼라 곱

```python
import numpy as np

A = np.array([[1, 2], [3, 4]])  # 2x2 행렬
B = np.array([[5, 6], [7, 8]])  # 2x2 행렬

C = A + B  # 행렬 덧셈
D = 2 * A  # 스칼라 곱

print(C)
print("-" * 50)
print(D)

# 출력 결과
# [[ 6  8]
#  [10 12]]
# --------------------------------------------------
# [[2 4]
#  [6 8]]

```

#### [Code] 행렬 곱 (Matrix Multiplication)

```python
import numpy as np

A = np.array([[1, 2, 3], [4, 5, 6]])
B = np.array([[7, 8], [9, 10], [11, 12]])

C = np.matmul(A, B) # 행렬 곱
print(C)

# 출력 결과
# [[ 58  64]
#  [139 154]]

```

### 선형 변환 (Linear Transformation)

* 행렬을 사용하여 공간의 모든 점을 새로운 위치로 대응시키는 변환.
* **평행 이동:** 공간의 모든 점을 같은 방향, 같은 거리만큼 이동 (선형 변환 아님).
* **데이터 전처리:** 데이터 분석의 전처리(센터링) 단계에서 사용.

---

## 3. Regression (회귀)

### 회귀란 무엇인가?

* 변수(특성)와 출력 변수(목표 값) 간의 선형 관계를 가정하여 예측하는 모델.
* 입력 데이터(독립 변수)를 기반으로 특정 연속적 목표 값(출력)을 예측하는 것이 목적.
* **입력 변수 (Features):** 집 크기, 위치, 방 개수 등.
* **출력 변수 (Target):** 예측하려는 연속적인 값 (예: 주택 가격).

### 회귀의 종류

* **단순 선형회귀:** 한 개의 독립변수로 종속변수를 예측 ().
* **다중 선형회귀:** 두 개 이상의 독립변수로 종속변수를 예측.
* **다항 회귀:** 2차항 이상의 항으로 계산, 비선형 패턴 모델링.
* *주의:* 과적합(Overfitting)의 위험이 있음 (차수가 높아질수록 모델이 복잡해짐).



#### [Code] Scikit-learn Linear Regression Example

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 캘리포니아 주택 가격 데이터셋 로드
data = fetch_california_housing()
df = pd.DataFrame(data.data, columns=data.feature_names) # 독립 변수 (특성 데이터)
df['HousePrice'] = data.target # 목표 변수 (주택 가격)

# 특성과 타겟 분리 (간단한 분석을 위해 일부 특성만 사용 가능하지만 여기서는 전체 로드 가정)
X = data.data
y = data.target

# 훈련 세트와 테스트 세트로 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 선형 회귀 모델 학습
model = LinearRegression()
model.fit(X_train, y_train)

# 예측 수행
y_pred = model.predict(X_test)

# 모델 평가
mse = mean_squared_error(y_test, y_pred) # 평균 제곱 오차 계산
r2 = r2_score(y_test, y_pred) # R-squared 계산

# 결과 출력
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')

```

---

## 4. 베이즈 정리 및 나이브 베이즈 분류

### 베이즈 정리 (Bayes' Theorem)

* 사건 B가 발생했을 때, A도 같이 발생했을 확률(사후 확률)을 구하는 정리.
* 공식: 

### 나이브 베이즈 분류 (Naive Bayes Classification)

* 베이즈 정리를 활용하여 입력값이 해당 클래스에 속할 확률을 계산하여 분류.
* **가정:** 각 특징(Feature)들이 서로 **독립**적이라고 가정함.
* **장점:** 학습 데이터가 적어도 잘 동작하며, 텍스트 분류(스팸 메일 필터링 등)에 효과적.
* **단점:** 특징들이 독립이 아니라면 예측 성능이 떨어질 수 있음.

---

## 5. Classification (분류) 개론

### 분류란 무엇인가?

* 입력 데이터를 미리 정의된 여러 카테고리(클래스) 중 하나에 속하도록 지정하는 작업.
* 주로 지도 학습(Supervised Learning) 방식으로 이루어짐.
* **예시:** 스팸 문자 분류(Spam or Ham), 암 판별, 숫자 이미지 인식 등.

### 결정 경계 (Decision Boundaries)

* 각 클래스에 속하는 데이터 포인트들을 구분하는 기준선.
* 선형 결정 경계(직선/평면) 또는 비선형 결정 경계(곡선 등)가 있음.

### 로지스틱 회귀 (Logistic Regression)

* 이름은 회귀지만 실제로는 **분류** 알고리즘.
* 선형 회귀의 결과를 **시그모이드(Sigmoid) 함수**를 통해 0과 1 사이의 확률 값으로 변환.
* 시그모이드 함수: 
* 가 커지면 1에 수렴, 작아지면 0에 수렴.



#### [Code] Scikit-learn Logistic Regression Example

```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# 데이터셋 로드 (Iris dataset)
data = load_iris()
X = data.data
y = data.target

# 훈련 세트와 테스트 세트로 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 모델 학습
model = LogisticRegression(max_iter=200)
model.fit(X_train, y_train)

# 예측 수행
y_pred = model.predict(X_test)

# 모델 평가
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

```

---

## 6. KNN (k-Nearest Neighbors)

### k-최근접 이웃 알고리즘

* 새로운 데이터 포인트가 주어졌을 때, 그 데이터와 가장 가까운 **k개의 이웃**을 기준으로 클래스를 결정하는 알고리즘.
* **작동 방식:**
1. 거리 계산: 새로운 데이터와 기존 데이터 간의 거리를 계산.
2. k개의 이웃 선택: 가장 가까운 k개의 이웃을 선택.
3. 다수결 투표: 선택된 이웃 중 가장 많은 클래스에 속하는지를 확인하여 분류.



### 장단점 및 사용 예시

* **장점:** 직관적이고 이해하기 쉬움. 복잡한 결정 경계도 학습 가능.
* **단점:** 데이터가 많아지면 계산 비용이 큼(게으른 학습, Lazy Learning). 차원의 저주에 취약함.
* **사용 예시:** 이미지 인식, 추천 시스템 등.

#### [Code] Scikit-learn KNN Example

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# 데이터셋 로드
data = load_iris()
X = data.data
y = data.target

# 훈련 세트와 테스트 세트로 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# KNN 모델 학습 (k=5 로 설정)
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)

# 예측 수행
y_pred = model.predict(X_test)

# 모델 평가
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

```