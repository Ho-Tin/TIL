제공해주신 동영상의 내용을 바탕으로 **데이터 분석을 위한 머신러닝 알고리즘 2**에 대한 내용을 마크다운(Markdown) 형식으로 정리해 드립니다. 요청하신 대로 코드는 요약하지 않고 포함하였습니다.

---

# 데이터 분석을 위한 머신러닝 알고리즘 2

이 챕터에서는 **SVM(Support Vector Machine), 앙상블(Ensemble), 군집(Clustering), Bias/Variance/Regularization**에 대해 다룹니다.

## 1. 서포트 벡터 머신 (SVM)

### 개요 및 특징

* **정의:** 서포트 벡터 머신은 클래스 간의 **여백(Margin)**을 최대화하는 초평면(Hyperplane)을 찾는 지도 학습 알고리즘입니다.
* **강점:** 딥러닝이 대두되기 전까지 분류와 회귀 문제 모두에서 사용할 수 있는 매우 강력한 모델로 평가받았습니다.
* **작동 방식:**
1. **초평면 결정:** 두 클래스 사이의 간격을 최대화하는 초평면을 찾습니다.
2. **서포트 벡터:** 초평면에 가장 가까운 데이터 포인트들을 서포트 벡터라고 하며, 이들이 결정 경계를 형성합니다.
3. **비선형 데이터 처리:** **커널 트릭(Kernel Trick)**을 사용하여 비선형 데이터를 고차원 공간으로 변환한 후, 선형 분리를 수행할 수 있습니다.



### Margin (여백)

* **Hard Margin:** 이상치(Outlier)를 허용하지 않고, 분명하게 나누는 결정 경계를 만듭니다. (과적합 위험, 노이즈에 민감)
* **Soft Margin:** 일부 이상치를 허용하여 유연한 결정 경계를 만듭니다.

### Kernel Trick (커널 트릭)

* 데이터를 고차원으로 보내서 서포트 벡터를 구하고 다시 저차원으로 축소하는 과정의 복잡한 연산을 줄이기 위한 기법입니다.
* 선형 분리가 불가능한 저차원 데이터를 고차원으로 보내 선형 분리를 가능하게 합니다.
* **대표적인 Kernel 함수:**
* Linear (선형 함수)
* Poly (다항식 함수)
* RBF (방사 기저 함수)
* Hyperbolic Tangent (쌍곡선 탄젠트)



### Scikit-learn 예제 코드

```python
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

# load dataset (using iris)
data = load_iris()
x = data.data
y = data.target

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# Model
model = SVC(kernel='linear')
model.fit(X_train, y_train)

```

---

## 2. 앙상블 (Ensemble)

### Tree 기반 모델 (Decision Tree)

* **장점:** 해석이 쉬움, 입력 값의 분포나 스케일에 영향을 덜 받음.
* **단점:** 예측력이 떨어질 수 있음, 과적합(Overfitting) 발생 가능성 높음.

### 앙상블 기법 개요

* 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식입니다.
* **보팅(Voting) 방식:**
* **Hard Voting:** 다수의 분류기가 예측한 결과값을 최종 결과로 선정.
* **Soft Voting:** 모든 분류기가 예측한 레이블 값의 결정 확률 평균을 구한 뒤 가장 확률이 높은 레이블 선정.



### 주요 앙상블 기법

1. **배깅 (Bagging - Bootstrap Aggregating):**
* 데이터 샘플링(Bootstrap)을 통해 여러 모델을 학습시키고 결과를 집계(Aggregation).
* 데이터 분할 시 중복을 허용합니다.
* 과적합 방지에 효과적입니다.
* **랜덤 포레스트 (Random Forest):**
* 서로 조금씩 다른 특성의 트리들로 구성 (앙상블).
* 각 트리의 예측이 서로 연관되지 않음(비상관성).
* 오류가 전파되지 않아 노이즈에 강함(건강성).




2. **부스팅 (Boosting):**
* 여러 개의 분류기가 순차적으로 학습을 수행합니다.
* 이전 분류기가 예측이 틀린 데이터에 대해 **가중치(Weight)**를 부여하여 다음 분류기가 더 잘 예측할 수 있도록 합니다.
* 대표 모델: XGBoost, AdaBoost, Gradient Boosting.



---

## 3. 군집 (Clustering)

### 개요

* 비슷한 특성을 가진 데이터들을 그룹화하는 **비지도 학습** 기법입니다.
* 데이터의 숨겨진 구조를 발견하거나 패턴을 파악하는 데 활용됩니다. (이상치 탐지, 추천 시스템 등)

### Cluster 간 거리 계산 방법 (계층적 군집 등)

1. **단일 연결법 (Simple Linkage):** 두 군집에서 가장 가까운 개체 간의 거리를 사용 (최단 연결).
2. **완전 연결법 (Complete Linkage):** 두 군집에서 가장 먼 개체 간의 거리를 사용 (최장 연결).
3. **Ward 연결법 (Ward's Linkage Method):** 군집 내의 오차 제곱합(Variance)의 증가량을 최소화하는 방식으로 군집을 병합.

### K-평균 (K-means clustering)

* 중심점(Centroid)을 기반으로 데이터를 군집화하는 알고리즘.
* 초기 중심점을 설정하고, 데이터들을 가장 가까운 중심점에 할당한 뒤, 중심점을 다시 계산하는 과정을 반복합니다.

---

## 4. Bias, Variance, Regularization

### Bias-Variance Trade-Off (편향-분산 트레이드오프)

* 머신러닝 모델의 총 예측 오차는 **편향(Bias)**, **분산(Variance)**, **불확실성(Noise)**의 합으로 구성됩니다.
* **편향(Bias):** 모델이 데이터의 복잡한 관계를 충분히 학습하지 못해 발생하는 오차 (Underfitting 위험).
* **분산(Variance):** 모델이 데이터의 작은 변동(노이즈)에 지나치게 민감하게 반응하여 발생하는 오차 (Overfitting 위험).


* **목표:** Bias와 Variance를 모두 최소화하여 일반화 성능을 극대화하는 것 (적절한 균형점 찾기).

### Regularization (정규화)

* 모델이 학습 데이터(Training data)에 너무 잘 맞추는 것(Overfitting)을 방지하기 위한 기법입니다.
* 손실 함수(Loss Function)에 페널티 항(Penalty term)을 추가합니다.
* **수식:** 
* **Data loss:** 모델의 예측이 학습 데이터와 일치해야 함.
* **Regularization term:** 모델이 학습 데이터에 '지나치게' 잘하는 것을 방지.



---

## 5. Standardization & Normalization

데이터 전처리 과정에서 피처(Feature)들의 스케일을 조정하는 방법입니다.

* **Normalization (정규화):**
* 데이터셋의 수치(numerical) 값의 범위를 0과 1 사이 등으로 조정하여 차이를 왜곡하지 않고 공통 척도로 변경.
* 데이터의 크기가 제각각이면 모델 학습이 비효율적이 될 수 있음.


* **Standardization (표준화):**
* 데이터가 평균 0, 분산 1을 갖는 표준 정규 분포의 속성을 갖도록 재조정.
* 모델이 데이터를 더 잘 학습할 수 있도록 함.